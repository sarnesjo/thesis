\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage{gnuplot-lua-tikz}

\usepackage{hyperref}
\hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{clrscode3e}
\renewcommand{\gets}{\leftarrow}

\newcommand{\AND}{\land}
\newcommand{\IOR}{\lor}
\newcommand{\XOR}{\oplus}
\newcommand{\NOT}{\lnot}

\title{Using binary decision diagrams to determine program equivalence in a superoptimizer}
\subtitle{}
\foreigntitle{Att använda binära beslutsdiagram för att avgöra ekvivalens mellan program i en superoptimerare}
\author{Jesper Särnesjö}
\date{}
\blurb{Master's Thesis at CSC \\ Supervisor: Torbjörn Granlund \\ Examiner: Johan Håstad}
\trita{TRITA xxx yyyy-nn}

% TODO: "BDD", not "OBDD"/"ROBDD"/"SROBDD"
% TODO: "target program", not "input program"

\begin{document}

\frontmatter

\pagestyle{empty}

\removepagenumbers

\maketitle

\selectlanguage{english}

\begin{abstract}
\end{abstract}

\clearpage

\begin{foreignabstract}{swedish}
\end{foreignabstract}

\clearpage

\tableofcontents*

\mainmatter

\pagestyle{newchap}

\chapter{Introduction}
\label{ch:introduction}

In the context of compiler technology, an \emph{optimizer} is a tool that attempts to improve the resource usage of programs given to it.
Typically, an optimizer takes a program and transforms it using heuristics designed, for instance, to reduce execution time or memory usage.
Despite its name, however, an optimizer rarely makes a program \emph{optimal}.

A \emph{superoptimizer} takes a different approach.
Given a \emph{target program} that computes a desired function, it attempts to find, typically using some type of exhaustive search, the fastest or shortest program that computes the same function.
In this way, a superoptimizer can generate programs that are indeed optimal.

Due to the inefficient nature of exhaustive search, a superoptimizer can only generate very short programs, typically no longer than 5 or 6 instructions, in reasonable time.
Further, a superoptimizer can only generate \emph{straight-line} programs, that is, programs that contain no loops, branches or other jumps.
Despite this, superoptimizers can be useful for optimizing inner loops and other short sequences of code required to be fast or small.

A central problem in the field of superoptimization, is that of determining whether a generated program computes the same function as the target program, in the sense that they always yield identical output given identical and valid input.
Programs that compute the same function are referred to as \emph{equivalent}.

Methods used to determine program equivalence in existing superoptimizers include probabilistic testing, as well as expressing programs in Boolean logic and comparing canonical representations of them.
A difficulty met by the latter method, is that the Boolean-logic representations of many common programs are very large.
This thesis explores using \emph{binary decision diagrams} (BDDs), which are data structures capable of efficiently representing Boolean functions, for the purpose of determining program equivalence.

Chapter~\ref{ch:background} provides a primer on the history and functionality of superoptimizers and BDDs.

Chapter~\ref{ch:target_architecture} describes the subset of the x86 architecture which serves as the target architecture for the superoptimizer implemented as part of this thesis project.

Chapter~\ref{ch:design_implementation} describes the superoptimizer in detail, focusing on its method of using BDDs to test programs for equivalence.

Chapter~\ref{ch:results_conclusions} presents an example of a program generated by the superoptimizer, as well as an analysis of the performance of the implementation of the BDD method.

Finally, Chapter~\ref{ch:recommendations_future_work} contains a few suggestions for improving on the results presented in this thesis.

% note on Boolean notation?

\chapter{Background}
\label{ch:background}

\section{Superoptimizers}

\subsection{Massalin's superoptimizer}

The term \emph{superoptimizer} was coined by Massalin in a 1987 paper \cite{massalin87}, to describe a tool for finding the shortest straight-line program that computes a given function.

Massalin's implementation takes as its input a target program written in the assembly language of the Motorola 68020 processor.
It then consults a table containing a subset of the processor's instruction set, and begins generating all combinations of these instructions, beginning with those of length 1, then 2, and so on.
Each generated program is tested for equivalence with the target program.
When a program passes the test, it is printed, and the search terminates.
Because programs are tested in order of increasing length, this program will necessarily be \emph{optimal} in terms of length, meaning that no shorter equivalent program exists.

Naturally, exhaustively searching the space of all possible programs is a very inefficient process, considering that the number of programs of length $n$ is $b^n$, where $b$ is the \emph{branching factor} of the search tree.
To find a value of $b$, consider an architecture with $r$ registers, and an instruction set consisting of $i_0$ instructions that take no arguments, $i_1$ instructions that take one argument, and $i_2$ instructions that take two arguments.
For this architecture, the branching factor would be, in the worst case, $i_0+i_1r+i_2r^2$.
Even for modest values of $i_0$, $i_1$, $i_2$ and $r$, the branching factor would end up in the hundreds or thousands.
Hence, a naive superoptimizer is only capable of generating very short programs in reasonable time.

To address this, Massalin describes a method of \emph{pruning} the search tree.
Using this method, the superoptimizer keeps a lookup table, where short instruction sequences known to be non-optimal are marked as such.
When a program is found to contain a non-optimal instruction sequence, it is rejected without being tested.

Massalin also describes two methods of determining whether a candidate program is equivalent to the target program.

The first method, referred to as the \emph{Boolean test}, is to express both programs in Boolean logic, reduce them to an unspecified canonical form, and compare them minterm for minterm.
Using this method, Massalin claims the superoptimizer is capable of testing 40 programs of an unspecified length per second, when running on a 16 MHz processor.
However, Massalin also notes that this method could not be used to test all generated programs in reasonable time, especially drawing attention to the large Boolean-logic representations of addition and multiplication instructions.

The second method, referred to as the \emph{probabilistic test}, is to simply execute both programs with identical input, and compare their output.
This method introduces the risk of false positives, in the form of programs that are equivalent only on the input used, not in the general case, making manual verification of the output necessary.
Further, whereas the Boolean test can be used on any kind of program, Massalin's probabilistic test can only be used on programs written in the assembly language of the host architecture.
This method is significantly faster, however, allowing the superoptimizer to test 50000 programs per second.

It is worth noting, that even with such a significant increase in speed, only slightly longer programs may be generated in reasonable time with a realistic branching factor, as illustrated by Figure~\ref{fig:so_program_length}.
Note that this is not a characteristic unique to Massalin's superoptimizer, but an intrinsic property of the exponentially large space of possible programs.

\begin{figure}
\centering
\include{so_program_length}
\caption{The maximum length of a program generated in 24 hours, as a function of the branching factor.
The solid and dashed lines represent Massalin's superoptimizer using the Boolean and probabilistic tests, respectively.}
\label{fig:so_program_length}
\end{figure}

\subsection{GSO}

GSO, the GNU superoptimizer, is described in a 1992 paper by Granlund and Kenner \cite{granlund92}.

Like Massalin's superoptimizer, GSO optimizes for program length.
It uses a probabilistic equivalence test, and as such its output must be manually verified.
Unlike the probabilistic test in Massalin's superoptimizer, which executes instructions directly on the host architecture, GSO's test \emph{simulates} them using functions that operate on a virtual machine of sorts, consisting of a set of registers and a single carry bit.
GSO is therefore capable of supporting a wide range of architectures, but also requires that the target program be specified as a compiled-in \emph{goal function}.

Granlund and Kenner also describe numerous methods used by GSO to prune the search tree.
When selecting arguments for generated instructions, GSO only considers live registers, that is, registers that contain input values or have previously been written to.
Similarly, instructions that read the carry flag are only generated after it has been set.
For commutative instructions, that is, instructions for which arguments can be ordered in different ways without changing the meaning of the instruction, only one argument ordering is tried.

When generating the last instruction in a program, GSO is even more restrictive.
Specifically, it requires that the last instruction reads a register or the carry flag written to by the preceding instruction, since that instruction would otherwise be superfluous.

\subsection{Denali}

Denali is a superoptimizer created by Joshi et al, then at Compaq Systems Research Center, first described in a 2002 paper \cite{joshi02}.

Denali's design differs greatly from that of Massalin's superoptimizer and GSO.
Rather than generating programs and then testing them, Denali starts by generating a set of programs equivalent to the target program, and then selects the optimal among them.

To accomplish the first step, Denali requires a set of \emph{axioms}, which describe how instructions may be substituted for another without changing the meaning of the program.
Such axioms may, for example, state that multiplication by $2^n$ is the same as right-shift by $n$.
Axioms are also used to mark instructions as commutative or associative, and to specify their identity, for instance 0 for addition and 1 for multiplication.
Denali applies these axioms to the target program, creating a set of programs equivalent to it.

The authors claim that Denali then selects, from this set, the program optimal in terms of \emph{execution time}, that is, the number of cycles required for the program to terminate.
This is remarkable, as determining the execution time for a program is complicated, with the number of cycles required to execute an instruction depending not only on the instruction itself, but also possibly on the instructions surrounding it, due to pipelining and other features of modern processors.
In a follow-up paper \cite{joshi06}, the authors describe a simplified design, named Denali-2, which instead optimizes for program length.

% input in somewhat C-like DSL
% multiple assignments
% Alpha EV-6
% uses external SAT-solver to select optimal program

\subsection{Bansal's superoptimizer}

The superoptimizer created by Bansal and described in his 2008 PhD thesis \cite{bansal_thesis} differs from the ones described above, in that it requires no human supervision, neither for verifying its output, nor for selecting its input.
Instead, it reads binaries compiled for the x86 architecture, harvesting instruction sequences to optimize.
Its output is not an optimized program, but rather the optimizations discovered.
In this way, Bansal's superoptimizer is capable of autonomously generating a very large library of optimizations, which can then be used by an ordinary peephole optimizer.

Candidate optimizations are tested for correctness using a probabilistic test similar to that found in Massalin's superoptimizer and GSO.
To eliminate the possibility of false positives, candidate optimizations which pass the probabilistic test are verified by being expressed in Boolean logic and made part of a satisfiability problem, using an unspecified method.
The satisfiability problem is then handed off to an external SAT solver.
No details of the performance of the Boolean verification process are given, although it is claimed that the superoptimizer can generate and verify programs containing 3 instructions in reasonable time.

Other notable features of Bansal's superoptimizer is that it is capable of optimizing both for execution time and for program size, and that it utilizes a meet-in-the-middle method of pruning the search tree.
Using this method, the superoptimizer searches not only \emph{forwards} for progressively longer programs, but also \emph{backwards} from the state the architecture is required to be in following correct program execution.

\subsection{TOAST}

TOAST, short for \emph{total optimisation using answer set technology}, is a superoptimizer created by Crick and described in his 2009 PhD thesis \cite{crick_thesis}.
It is implemented using Answer Set Programming, a declarative logic programming language.
Like Denali and Bansal's superoptimizer, it produces provably correct output.

%\subsection{...}
%
%\cite{aha}
%\cite{pic}

\section{Binary decision diagrams}
\label{s:bdds}

\emph{Binary decision diagrams} (BDDs) are data structures used to represent Boolean functions.
They are directed acyclic graphs, with one or more nodes for each of the function's variables.
Each node has one or more incoming paths, and exactly two outgoing paths, commonly referred to as the \emph{high} and \emph{low} path.
When using a BDD to determine the value of a Boolean function, one follows a node's high path if its variable is 1, and its low path if its variable is 0, starting at the root node, until one arrives at a terminal node.

% if-then-else nodes
% any boolean function can be represented as a BDD

BDDs can be used to store and manipulate some Boolean functions very efficiently.
However, the performance of the operations performed on a BDD, varies greatly depending on the function represented, and the order of its variables.

BDDs were introduced by Lee \cite{lee59}, under the name \emph{binary-decision programs}.
They were given their current name by Akers \cite{akers78}, who also explored the ideas of \emph{reducing} BDDs by removing redundant nodes, and of representing multiple functions in a single BDD.

The concept of reducing BDDs was formalized by Bryant \cite{bryant86}, who described an efficient algorithm that leaves a BDD containing no node with the same high and low path, and no two distinct nodes with isomorphic subgraphs, while still representing the same Boolean function.
Bryant observed that a BDD that is both \emph{reduced} and \emph{ordered}, meaning that its variables appear in the same order on all paths from the root to a terminal node, is a \emph{canonical} represention of its Boolean function.
In other words, two Boolean functions are equivalent iff their reduced and ordered BDD representations are identical.

Bryant also described a set of algorithms for combining BDDs using Boolean operators to create BDD representations of more complex Boolean functions.
These algorithms are efficient, requiring at worst a number of time steps proportional to the product of the number of nodes in the two BDDs.

Bryant further explored the idea of representing multiple functions in a single BDD, with common subexpressions reduced.
This representation would later be referred to as a \emph{shared} BDD by Minato et al \cite{minato90}, who also noted that two equivalent Boolean functions represented in the same shared BDD, would have the same root node, making an equivalence test trivial.

Bryant further showed that the variable ordering used can have a dramatic effect on the number of nodes in a reduced BDD, which in the best case is linear in the number of variables, and in the worst case exponential.
Bryant states that finding the variable ordering that minimizes the number of nodes is an NP-complete problem.
Bollig and Wegener \cite{bollig96} would later show that even determining whether an ordering exists for which the number of nodes is at most a given value, is an NP-complete problem.

Bryant asserts that for many functions, a good variable ordering can be determined by examining the structure of the functions.
As an example, he mentions that for functions in which variables primarily interact pairwise, ordering variables and their partners consecutively gives good results.

However, Bryant also states that there are functions that can only be represented using a number of nodes exponential in the number of variables, regardless of variable ordering, and proves that integer multiplication is such a function.

% TODO: lower bounds given by Bollig and Wegener

\subsection{Example}

To illustrate how a moderately complex Boolean function can be represented by a BDD, consider an adder that takes two $n$-bit numbers, $a$ and $b$, and produces their sum, as well a carry-out bit.

To express the adder in Boolean logic, we represent $a$ as $n$ Boolean variables $\langle a_0, a_1, ..., a_{n-1} \rangle$, where $a_0$ and $a_{n-1}$ are the least and most significant bits of $a$, respectively.
Doing the same for $b$, we get a total of $2n$ Boolean variables.

The function for the carry-out bit, $c_{n-1}$, depends on every variable, and can be defined recursively as follows:

$$
  c_k = \left\{
  \begin{array}{ll}
    a_k \AND b_k                                             & k = 0 \\
    a_k \AND b_k \IOR a_k \AND c_{k-1} \IOR b_k \AND c_{k-1} & 0 < k < n \\
  \end{array}\right.
$$

For $n = 2$, the function $c_1$ determines the carry-out bit of the adder.
The truth table of $c_1$ is shown in Table~\ref{tab:tt_c1}, while Figure~\ref{fig:bdd_c1} shows $c_1$ as represented by an ordered BDD, using the variable ordering $a_0,b_0,...,a_{n-1},b_{n-1}$.
Note that the row of terminal nodes in the BDD correspond to the column for $c_1$ in the truth table.
Figure~\ref{fig:bdd_c1} also shows a reduced version of the same BDD.
Note that this representation is quite compact, and that not all variables are present on every path.
For instance, if $a_0$ is 0, there is no need to check $b_0$, and we instead skip ahead to $a_1$.

Not all variable orderings allow such compact BDD representations of this function, however.
If we instead use the variable ordering $a_0,...,a_{n-1},b_0,...,b_{n-1}$, we end up with a larger BDD.
This becomes more clear for larger values of $n$, as illustrated in Figure~\ref{fig:bdd_c2_bad}.

Upon inspection, we find that in the reduced BDD representation of this particular function, using these two variable orderings, the number of nodes is:

$$
  |c_{n-1}| = \left\{
  \begin{array}{ll}
    3n-1      & \textrm{using variable order $a_0,b_0,...,a_{n-1},b_{n-1}$} \\
    2^{n+1}-2 & \textrm{using variable order $a_0,...,a_{n-1},b_0,...,b_{n-1}$} \\
  \end{array}\right.
$$

For $n = 32$, the reduced BDD would consequently contain only 97 nodes with the first variable ordering, but over 8 billion with the second.

Note that Bryant's suggested method of ordering interacting variables consecutively, works very well in this case.

\begin{table}[p]
\centering
\begin{tabular}{cccc|cc}
$a_0$ & $b_0$ & $a_1$ & $b_1$ & $c_0$ & $c_1$ \\
\hline
0     & 0     & 0     & 0     & 0     & 0 \\
0     & 0     & 0     & 1     & 0     & 0 \\
0     & 0     & 1     & 0     & 0     & 0 \\
0     & 0     & 1     & 1     & 0     & 1 \\
0     & 1     & 0     & 0     & 0     & 0 \\
0     & 1     & 0     & 1     & 0     & 0 \\
0     & 1     & 1     & 0     & 0     & 0 \\
0     & 1     & 1     & 1     & 0     & 1 \\
1     & 0     & 0     & 0     & 0     & 0 \\
1     & 0     & 0     & 1     & 0     & 0 \\
1     & 0     & 1     & 0     & 0     & 0 \\
1     & 0     & 1     & 1     & 0     & 1 \\
1     & 1     & 0     & 0     & 1     & 0 \\
1     & 1     & 0     & 1     & 1     & 1 \\
1     & 1     & 1     & 0     & 1     & 1 \\
1     & 1     & 1     & 1     & 1     & 1 \\
\end{tabular}
\caption{Truth table for the Boolean function $c_1$.}
\label{tab:tt_c1}
\end{table}

\begin{figure}[p]
\centering
\include{bdd_c1}
\caption{An ordered BDD representation of the Boolean function $c_1$ (left), and its canonical reduced representation (right). The solid lines represent high paths and the dashed lines represent low paths.}
\label{fig:bdd_c1}
\end{figure}

\begin{figure}[p]
\centering
\include{bdd_c2_bad}
\caption{Reduced BDD representation of the Boolean function $c_2$ using the variable order $a_0,a_1,a_2,b_0,b_1,b_2$ (left) and $a_0,b_0,a_1,b_1,a_2,b_2$ (right).}
\label{fig:bdd_c2_bad}
\end{figure}

\chapter{Target architecture of the superoptimizer}
\label{ch:target_architecture}

The superoptimizer implemented as part of this thesis project targets the x86 architecture, as described by the manuals published by Intel Corporation \cite{intel_1,intel_2a,intel_2b}.

Since the x86 architecture is large and quite complex, it should be stressed that the superoptimizer only supports a very small subset of its features.
While there are hundreds of instructions in the x86 general purpose instruction set, and many more in its numerous extensions, the superoptimizer's instruction set contains only 33 instructions.
These instructions operate only on registers and flags, passing in immediate values or addressing the memory is not supported.

% all registers same size, not all flags

The assembly language used to describe programs accepted as input or produced as output by the superoptimizer is similar, but not identical, to the one used by Intel.
Programs generated by the superoptimizer can, with minor syntactical modifications, be assembled and executed on a real x86 machine.

% this chapter describes architecture in brief and to aid understanding of superoptimizer operation
% not intended as programming guide nor complete description
% complete and in-depth information can be found in Intel's manuals.

\section{Registers}

Registers are used to hold integer values, and are read from and written to by instructions.
Two's-complement arithmetic is used to represent negative numbers, but since registers are typeless, whether a given register contains a signed or an unsigned integer is determined solely by how it is used.
Indeed, registers do not have to represent integers, but can be seen simply as sets of bits.

There are 8 registers in the superoptimizer's architecture, each 32 bits in size.

\section{Flags}
\label{s:flags}

Flags are single-bit values used to signify various conditions.
They are written to and read from by instructions, in general as a side effect rather than directly.

There are 17 different flags in the x86 architecture, but the superoptimizer only supports the 5 for which a corresponding \verb|cmov| instruction exists:
the \emph{carry} (\verb|cf|), \emph{overflow} (\verb|of|), \emph{parity} (\verb|pf|), \emph{sign} (\verb|sf|) and \emph{zero} (\verb|zf|) flags.

\verb|cf| and \verb|of| are used to signify that overflow has occurred, meaning that the result of an operation had to be truncated to fit into a single register.
\verb|cf| is used for unsigned arithmetic, while \verb|of| is used for signed arithmetic, but their exact meaning varies from instruction to instruction.

\verb|pf|, \verb|sf| and \verb|zf| can be determined entirely by inspecting the result of an operation.
\verb|pf| signifies that an even number of the least significant 8 bits of the result are set to 1,
\verb|sf| that the result is negative if interpreted as a signed number, since the most significant bit is 1,
and \verb|zf| that every bit of the result is 0.

\section{Language}
\label{s:language}

Programs accepted as input or produced as output by the superoptimizer, are written in assembly language, using Intel's syntax with a few modifications.
Registers are referred to as \verb|r0| through \verb|r7|, rather than the usual names \verb|eax|, \verb|ebx|, etc.
Statements are terminated by semicolons rather than newlines.

For instructions that take one or two arguments, the first specifies the \emph{destination} register, while the second specifies the \emph{source} register.
The instruction may read from the destination register, write to it, or do both.
The source register, however, is only read from.

An example of a program written in the superoptimizer's assembly language is the following, which sets \verb|r0| to the sum of \verb|r1| and \verb|r2|:

\begin{verbatim}
mov r0,r1; add r0,r2;
\end{verbatim}

\section{Instruction set}

When selecting a set of instructions for a superoptimizer to support, some care should be taken.
Only instructions which are likely to be used should be included, to avoid increasing the branching factor of the superoptimizer's search tree.
Further, since some instructions take longer than others to execute on an x86 architecture \cite{x86_timing}, one might wish to avoid including slower instructions, to ensure that a program optimal in terms of length, also performs well in terms of execution time.

Since the focus of this thesis project is on testing programs for equivalence, however, the set of supported instructions was chosen to exhibit variety in terms of behavior with regards to registers and flags, and to demonstrate both the strengths and the weaknesses of the method used. The supported instructions are listed in Table~\ref{tab:insns}.

\begin{table}
\centering
\begin{tabular}{l|cc|ccccc}
instruction   & d  & s  & \verb|cf| & \verb|of| & \verb|pf| & \verb|sf| & \verb|zf| \\
\hline
\verb|stc|    &    &    & W         &           &           &           &           \\
\verb|clc|    &    &    & W         &           &           &           &           \\
\verb|cmc|    &    &    & RW        &           &           &           &           \\
\hline
\verb|mov|    & W  & R  &           &           &           &           &           \\
\verb|cmovc|  & RW & R  & R         &           &           &           &           \\
\verb|cmovo|  & RW & R  &           & R         &           &           &           \\
\verb|cmovp|  & RW & R  &           &           & R         &           &           \\
\verb|cmovs|  & RW & R  &           &           &           & R         &           \\
\verb|cmovz|  & RW & R  &           &           &           &           & R         \\
\verb|cmovnc| & RW & R  & R         &           &           &           &           \\
\verb|cmovno| & RW & R  &           & R         &           &           &           \\
\verb|cmovnp| & RW & R  &           &           & R         &           &           \\
\verb|cmovns| & RW & R  &           &           &           & R         &           \\
\verb|cmovnz| & RW & R  &           &           &           &           & R         \\
\verb|cmova|  & RW & R  & R         &           &           &           & R         \\
\verb|cmovbe| & RW & R  & R         &           &           &           & R         \\
\verb|cmovg|  & RW & R  &           & R         &           & R         & R         \\
\verb|cmovge| & RW & R  &           & R         &           & R         &           \\
\verb|cmovl|  & RW & R  &           & R         &           & R         &           \\
\verb|cmovle| & RW & R  &           & R         &           & R         & R         \\
\hline
\verb|and|    & RW & R  & W         & W         & W         & W         & W         \\
\verb|or|     & RW & R  & W         & W         & W         & W         & W         \\
\verb|xor|    & RW & R  & W         & W         & W         & W         & W         \\
\verb|not|    & RW &    &           &           &           &           &           \\
\hline
\verb|add|    & RW & R  & W         & W         & W         & W         & W         \\
\verb|adc|    & RW & R  & RW        & W         & W         & W         & W         \\
\verb|sub|    & RW & R  & W         & W         & W         & W         & W         \\
\verb|sbb|    & RW & R  & RW        & W         & W         & W         & W         \\
\verb|cmp|    & R  & R  & W         & W         & W         & W         & W         \\
\verb|inc|    & RW &    &           & W         & W         & W         & W         \\
\verb|dec|    & RW &    &           & W         & W         & W         & W         \\
\verb|neg|    & RW &    & W         & W         & W         & W         & W         \\
\hline
\verb|imul|   & RW & R  & W         & W         & U         & U         & U         \\
\end{tabular}
\caption{The instruction set of the superoptimizer.
The columns marked \emph{d} and \emph{s} describe the instruction's behaviour with regards to its destination and source register, respectively,
while the columns marked \emph{cf}, \emph{of}, \emph{pf}, \emph{sf} and \emph{zf} describe its behaviour with regards to flags.
\emph{R} and \emph{W} mark a register or flag as read from or written to, respectively, while \emph{U} marks a flag as left in an undefined state by an instruction.} % typography
\label{tab:insns}
\end{table}

\subsection{Flag control instructions}

The simplest supported instructions are the flag control instructions.
\verb|stc| and \verb|clc| set \verb|cf| to 1 and 0, respectively.
\verb|cmc| complements \verb|cf|, flipping it from 0 to 1 or vice versa.
These instructions do not modify registers in any way.

\subsection{Data transfer instructions}

The data transfer instructions copy the value of the source register to the destination register.
\verb|mov| does so unconditionally, while the \verb|cmov| family of instructions does so only if a given condition is true, otherwise leaving the destination register unmodified.
There are 16 \verb|cmov| instructions in the superoptimizer's instruction set.

\verb|cmovc|, \verb|cmovo|, \verb|cmovp|, \verb|cmovs| and \verb|cmovz|, as well as \verb|cmovnc|, \verb|cmovno|, \verb|cmovnp|, \verb|cmovns| and \verb|cmovnz|, only inspect the value of a single flag, transferring data if that flag is set to an expected value.
\verb|cmova|, \verb|cmovbe|, \verb|cmovg|, \verb|cmovge|, \verb|cmovl| and \verb|cmovle|, on the other hand, are slightly more complex, since they inspect the values of several flags, comparing them not only to expected values, but also to each other.

% above/below for unsigned
% greater/less for signed

% cmovc/cmovb/cmovnae  carry/below/not above or equal
% cmovo                overflow
% cmovp/cmovpe         parity/parity even
% cmovs                sign
% cmovz/cmove          zero/equal
% cmovnc/cmovae/cmovnb not carry/above or equal/not below
% cmovno               not overflow
% cmovnp/cmovpo        not parity/parity odd
% cmovns               not sign
% cmovnz/cmovne        not zero/not equal
% cmova/cmovnbe        above/not below or equal
% cmovbe/cmovna        below or equal/not above
% cmovg/cmovnle        greater/not less or equal
% cmovge/cmovnl        greater or equal/not less
% cmovl/cmovnge        less/not greater or equal
% cmovle/cmovng        less or equal/not greater

As shown in Table~\ref{tab:insns}, \verb|mov| is considered to read from one register and write to another, while the \verb|cmov| instructions are considered to read from both, and write to one.
This, together with a requirement that instructions only read from registers with defined values, ensures that the value of the destination register remains well-defined.

The data transfer instructions do not modify any flags.

\subsection{Logic instructions}

The instruction set contains four instructions that perform bitwise Boolean logic.

\verb|and|, \verb|or| and \verb|xor| all take two arguments, and perform bitwise logical \emph{and}, \emph{inclusive-or} and \emph{exclusive-or}, respectively.
These instructions always set \verb|cf| and \verb|of| to 0, while \verb|pf|, \verb|sf| and \verb|zf| are set based on the result as described in Section~\ref{s:flags}.

\verb|not| takes a single argument and performs bitwise logical negation on it.
It does not modify any flags.

\subsection{Addition and subtraction instructions}

A large number of the supported instructions perform some variant of addition or subtraction.

The most basic of these is \verb|add|, which takes two arguments and performs integer addition on them.
Since two's-complement arithmetic is used, \verb|add| could be considered to perform both signed and unsigned addition simultaneously, with \verb|cf| and \verb|of| set to indicate unsigned and signed overflow, respectively.
\verb|pf|, \verb|sf| and \verb|zf| are set based on the result as described in Section~\ref{s:flags}.

\verb|adc| performs addition with \emph{carry-in}, meaning that it adds not only its two arguments, but also \verb|cf|.

\verb|sub| and \verb|sbb| are the subtraction counterparts of \verb|add| and \verb|adc|.
\verb|cmp| behaves the same as \verb|sub| with regards to flags, but does not write its result to a register.

\verb|inc| and \verb|dec| behave like \verb|add| and \verb|sub|, but take only one argument, substituting the value 1 for the second argument.
They do not alter the value of \verb|cf|, but modify the other flags in the same way as \verb|add|.

\verb|neg| takes one arguments and performs two's-complement negation on it.
It sets \verb|cf| to 0 if its argument is 0, and 1 otherwise, and modifies the other flags in the same way as \verb|add|.

\subsection{Multiplication instructions}

The x86 instruction set contains many multiplication instructions, each with slightly different behavior.
The only instruction supported by the superoptimizer is the form of \verb|imul| that takes two arguments and performs integer multiplication on them, truncating the result to fit into a single register.

\verb|cf| and \verb|of| are both used to indicate that the result of the multiplication did not fit into a single register prior to being truncated, meaning that overflow has occurred.
\verb|pf|, \verb|sf| and \verb|zf|, however, are left in an \emph{undefined} state.

\chapter{Design and implementation of the superoptimizer}
\label{ch:design_implementation}

The superoptimizer implemented as part of this thesis project takes as its input a target program written in the assembly language described in Section~\ref{s:language}.
It then begins generating candidate programs in the same assembly language, testing them for equivalence with the target program.
Programs are generated in order of increasing length, meaning that the first program found to be equivalent to the target program, will be optimal in terms of length.
The superoptimizer continues generating and testing programs until it reaches a user-specified maximum length, or is terminated by the user.

% design primarily inspired by GSO?

The superoptimizer was implemented using C and C++.
A suite of unit tests was created to aid development, but no formal verification of correctness was performed.

\section{Determining register usage}

The first action performed by the superoptimizer, is to determine the input and output registers of the target program.
This information is used both when generating candidate programs, and when determining whether they are equivalent to the target program.

Any register that is read from before it is written to by the target program, is marked as an input register.
Input registers are assumed to contain well-defined values at the start of the program's execution.
Candidate programs generated by the superoptimizer are allowed, but not required, to read from these registers.

The last register written to by the target program, is marked as the output register.
Generated programs are required to use the same output register as the target program.
Indeed, the superoptimizer determines equivalence based solely on the output register, ignoring the values of all other registers as well as the flags.
Programs which compute the same function, but write the result to different registers, are not considered equivalent.

\section{Generating candidate programs}

The superoptimizer uses an iterative deepening depth-first search to find programs equivalent to the target program.
Initially, the depth limit is 1, meaning that programs of of length 1 are generated.
Once all programs of length 1 have been generated or pruned, the depth limit is increased to 2, and so on.
This continues until a user-specified maximum length has been reached, or the superoptimizer is terminated by the user.

% neither 0-length programs nor nop supported, xo can't optimize 'and r0,r0;'

The superoptimizer prunes the search tree by not generating some types of programs which cannot possibly be equivalent to the target program.

In particular, the superoptimizer does not generate programs that read from registers or flags whose values are not well-defined. % explain why?
For this, the superoptimizer keeps a \emph{live set} of registers, which initially consists of the input registers, as well as one of flags, which initially is empty.
Only instructions that read from registers in the live set, or instructions that read no registers at all, are generated.
Similarly, instructions that read flags are only generated if those flags are in the live set.
After generating an instruction that writes to a register or flags, the superoptimizer adds that register or those flags to their respective live sets.
% registers never removed from live set
% flags can be (imul)

A few instructions are given special treatment by the superoptimizer.

\verb|xor|, \verb|sub| and \verb|sbb| can all be used to set a register to a well-defined value, regardless of the register's previous value, by using that register both as the destination and source register.
The superoptimizer therefore has a special rule which permits, but does not require, these instructions to read from, and write to, a register not in the live set.

% \verb|xor| and \verb|sub| -> 0
% \verb|sbb| -> 0 or 1111...

For \verb|mov| and the \verb|cmov| instructions, the destination register is required to be different from the source register, since the instruction would otherwise have no effect.
For \verb|mov|, the destination register is also required to be a register \emph{not} in the live set, or the program's output register.
While \emph{allowing} \verb|mov| to write to a register not in the live set is necessary, since there would otherwise be no way of making a copy of a value required by multiple instructions,
it should be noted that \emph{requiring} it to do so may cause the superoptimizer to generate programs that use too many registers unnecessarily.

When generating the very last instruction in a candidate program, the superoptimizer requires that the instruction writes to the program's output register, since the instruction would otherwise have no effect.
% note! does not use GSO pruning that last insn must read reg or flag written to by preceding insn

% as a note, the code for the program generator grew quite messy over time, and that it is likely that it contains some bugs.

\section{Determining equivalence with the target program}

Each generated candidate program is tested for equivalence with the target program using a probabilistic test similar to the one used by GSO.
This test is fast, but may report false positives.
Programs that pass the probabilistic test are therefore verified by being expressed as Boolean-logic functions represented by BDDs.
This verification is slower than the probabilistic test, but yields correct results, barring any bugs in its implementation.

The probabilistic test and BDD-based method of verification can be seen as two independent implementations of the architecture defined in Chapter~\ref{ch:target_architecture}.
If should be noted, however, that neither of them are capable of correctly handling programs which read from undefined registers or flags, since such programs are not expected to be generated.

\subsection{Testing for equivalence probabilistically}
\label{ss:probabilistic_test}

The superoptimizer's probabilistic method of testing a candidate program for equivalence with the target program, is to simply execute both programs with identical input and compare their output.

Like GSO, the superoptimizer implemented as part of this thesis project does not execute the instructions in a program directly on the host architecture, but rather simulates them.
These simulated instructions, which are written in C, operate on registers represented by unsigned integers and flags represented by single bits.
Since the instructions are generally simple enough to be implemented in a straight-forward manner using only a few C operators, they will not be described in further detail.

When testing a candidate program for equivalence with the target program, the superoptimizer uses two sets of registers and flags, with the registers in the two sets given identical values taken from a hardcoded test vector.
% The flags are initially set to 0, although no program is expected to read from flags which have not already been written to.

The superoptimizer executes the candidate and target program, one on each set of registers and flags, and then compares the values of the output registers in the two sets.
If they differ, the programs are certainly not equivalent, and the test ends.
If they are equal, the test starts over, using new values taken from the test vector.
When all values in the test vector have been used, the candidate program passes the test, as it is equivalent to the target program at least for the values used.

Since it is still possible that the candidate program is in fact not equivalent to the target program in the general case, one might ask why the programs are not tested using \emph{all} possible values.
Indeed, for a program with a single 32-bit input register, there are only $2^{32}$ possible values to test.
Considering that programs generated by a superoptimizer are no longer than 5 or 6 instructions, running them $2^{32}$ times should take less than a minute on a modern computer.

However, this method will not work well even for programs with two input registers.
Such a program would need to be run $2^{64}$ times, which would take several thousand years.

% (2^32 * 5 * 3) / (2 GHz) = 32 seconds
% (2^64 * 5 * 3) / (2 GHz) = 4384 years

\subsection{Verifying equivalence using BDDs}
\label{ss:bdd_test}

To determine if two programs are equivalent, one can express both in Boolean logic, reduce them to some canonical form, and compare them.
As described in Section~\ref{s:bdds}, reduced and ordered BDDs are canonical representations of Boolean functions, and can therefore be used for this purpose.

The superoptimizer's method of determining program equivalence using BDDs was implemented in C++, using the software package BuDDy \cite{buddy}.
BuDDy was chosen out of a small pool of alternatives, for its reasonably well-written documentation and apparent popularity.

\subsubsection{Overview}

The superoptimizer keeps a reference to a BDD representing a Boolean function for each bit in each register, and for each flag.
All bits and flags are represented in a single, shared BDD.
Initially, the bits in a register $a$ reference the variables $\langle a_0, a_1, ..., a_{n-1} \rangle$, where $n$ is the number of bits in the register.
The variables are ordered so that those which represent bit 0 in a register all come before those which represent bit 1, and so on, which was found to work well in practice.

When creating a BDD representation of a program, the superoptimizer steps through its instructions in order.
For each instruction, the superoptimizer performs Boolean operations on the existing BDDs, changing the references in the instruction's destination register to point to new BDDs.
With each operation, the shared BDD is reduced, which takes some time.

The initial state of the BDD model, as well as the state after applying an addition instruction, are shown in a simplified way in Figure~\ref{fig:bdd_add}.
Note that the registers in the figure only contain 2 bits, and that not all references are shown.
The implementation of the addition instruction is detailed below.

\begin{figure}
\centering
\include{bdd_add1}
\include{bdd_add2}
\caption{A simplified 2-bit version of the BDD model used by the superoptimizer in its initial state (top) and after applying the function \proc{add}(a, b) (bottom).
$a'_0$ and $a'_1$ mark the BDDs currently representing bits 0 and 1 in register $a$, while $f_c$ marks the BDD representing the carry flag.
$a_0$, $a_1$, $b_0$ and $b_1$, are the variables initially referenced by the bits in $a$ and $b$.
Neither the BDDs representing the bits in $b$, nor those representing the other flags, are marked.}
\label{fig:bdd_add}
\end{figure}

Once the superoptimizer has stepped through all instructions in the program, the bits in the program's output register will reference BDDs that are canonical representations of Boolean functions on the initial variables.

To determine if a candidate program is equivalent to the target program, the superoptimizer performs the process described above for both programs, and then compares the references to the BDDs representing the bits in their output registers pairwise.
If all references are equal, the two programs are equivalent.

For each instructions in the superoptimizer's instructions set, a corresponding function was created to apply the equivalent Boolean-logic operations on the registers and flags.
In the descriptions of these functions below, a register $a$ is represented by $n$ Boolean variables $\langle a_0, a_1, ..., a_{n-1} \rangle$, where $a_0$ and $a_{n-1}$ are the least and most significant bits of $a$, respectively.
The flags are represented as $f_c$, $f_o$, $f_p$, $f_s$ and $f_z$, and are treated as global variables.

\subsubsection{Flag control instructions}

The flag control instructions are trivial to express in Boolean logic, with \proc{stc} and \proc{clc} setting $f_c$ to 1 and 0, respectively, and \proc{cmc} setting $f_c$ to its logical negation:

\begin{codebox}
\Procname{$\proc{stc}()$}
\zi $f_c \gets 1$
\end{codebox}

\begin{codebox}
\Procname{$\proc{clc}()$}
\zi $f_c \gets 0$
\end{codebox}

\begin{codebox}
\Procname{$\proc{cmc}()$}
\zi $f_c \gets \NOT f_c$
\end{codebox}

\subsubsection{Data transfer instructions}

The \proc{mov} instruction is straight-forward, since it contains no BDD operations, and consists only of an array assignment:

\begin{codebox}
\Procname{$\proc{mov}(a,b)$}
\zi \For $i \gets 0$ \To $n-1$
\zi \Do
      $a_i \gets b_i$
    \End
\end{codebox}

The \proc{cmov} instructions are slightly more complex.
Using Boolean logic, a conditional value can be expressed as $\NOT c \AND a \IOR c \AND b$, where $c$ is the condition, and $a$ and $b$ the two possible values.
Since the condition is the same for each bit, the \proc{cmov} instructions all have the following form, with different definitions of \id{condition}:

\begin{codebox}
\Procname{$\proc{cmov}(a,b)$}
\zi $c \gets \id{condition}$
\zi \For $i \gets 0$ \To $n-1$
\zi \Do
      $a_i \gets \NOT c \AND a_i \IOR c \AND b_i$
    \End
\end{codebox}

\subsubsection{Logic instructions}

The logic instructions are similarly straight-forward, since each bit $i$ in the result depends only the corresponding bit $i$ in the input or inputs.

\proc{and}, \proc{or} and \proc{xor} are all similar in form:

\begin{codebox}
\Procname{$\proc{and}(a,b)$}
\zi \For $i \gets 0$ \To $n-1$ \Do
\zi   $a_i \gets a_i \AND b_i$ \End
\zi $f_c \gets 0$
\zi $f_o \gets 0$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\begin{codebox}
\Procname{$\proc{or}(a,b)$}
\zi \For $i \gets 0$ \To $n-1$ \Do
\zi   $a_i \gets a_i \IOR b_i$ \End
\zi $f_c \gets 0$
\zi $f_o \gets 0$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\begin{codebox}
\Procname{$\proc{xor}(a,b)$}
\zi \For $i \gets 0$ \To $n-1$ \Do
\zi   $a_i \gets a_i \XOR b_i$ \End
\zi $f_c \gets 0$
\zi $f_o \gets 0$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\proc{not} is even simpler, in that it only takes a single argument:

\begin{codebox}
\Procname{$\proc{not}(a)$}
\zi \For $i \gets 0$ \To $n-1$ \Do
\zi   $a_i \gets \NOT a_i$ \End
\end{codebox}

\subsubsection{Addition and subtraction instructions}

The instructions that perform addition or subtraction are somewhat more complex, since each bit $i$ in the result depends on bits $0--i$ in the input or inputs.
They are all implemented as \emph{ripple-carry adders}, with $c_i$ being the carry-out of position $i$.

\proc{add} and \proc{adc} are defined similarly, differing only in that \proc{adc} uses $f_c$ as a \emph{carry-in}:

\begin{codebox}
\Procname{$\proc{add}(a,b)$}
\zi $c_0 \gets a_0 \AND b_0$
\zi $a_0 \gets a_0 \XOR b_0$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets a_i \AND b_i \IOR a_i \AND c_{i-1} \IOR b_i \AND c_{i-1}$
\zi   $a_i \gets a_i \XOR b_i \XOR c_{i-1}$ \End
\zi $f_c \gets c_{n-1}$
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\begin{codebox}
\Procname{$\proc{adc}(a,b)$}
\zi $c_0 \gets a_0 \AND b_0 \IOR a_0 \AND f_c \IOR b_0 \AND f_c$
\zi $a_0 \gets a_0 \XOR b_0 \XOR f_c$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets a_i \AND b_i \IOR a_i \AND c_{i-1} \IOR b_i \AND c_{i-1}$
\zi   $a_i \gets a_i \XOR b_i \XOR c_{i-1}$ \End
\zi $f_c \gets c_{n-1}$
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\proc{sub} and \proc{sbb} are also very similar:

\begin{codebox}
\Procname{$\proc{sub}(a,b)$}
\zi $c_0 \gets \NOT a_0 \AND b_0$
\zi $a_0 \gets a_0 \XOR b_0$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets \NOT a_i \AND b_i \IOR \NOT a_i \AND c_{i-1} \IOR b_i \AND c_{i-1}$
\zi   $a_i \gets a_i \XOR b_i \XOR c_{i-1}$ \End
\zi $f_c \gets c_{n-1}$
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\begin{codebox}
\Procname{$\proc{sbb}(a,b)$}
\zi $c_0 \gets \NOT a_0 \AND b_0 \IOR \NOT a_0 \AND f_c \IOR b_0 \AND f_c$
\zi $a_0 \gets a_0 \XOR b_0 \XOR f_c$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets \NOT a_i \AND b_i \IOR \NOT a_i \AND c_{i-1} \IOR b_i \AND c_{i-1}$
\zi   $a_i \gets a_i \XOR b_i \XOR c_{i-1}$ \End
\zi $f_c \gets c_{n-1}$
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\proc{cmp} is defined almost exactly as \proc{sub}, with the only difference being that the result of the subtraction is stored in a temporary register $t$, which is used to when setting $f_p$, $f_s$ and $f_z$:

\begin{codebox}
\Procname{$\proc{cmp}(a,b)$}
\zi $c_0 \gets \NOT a_0 \AND b_0$
\zi $t_0 \gets a_0 \XOR b_0$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets \NOT a_i \AND b_i \IOR \NOT a_i \AND c_{i-1} \IOR b_i \AND c_{i-1}$
\zi   $t_i \gets a_i \XOR b_i \XOR c_{i-1}$ \End
\zi $f_c \gets c_{n-1}$
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(t_0 \XOR t_1 \XOR ... \XOR t_7)$
\zi $f_s \gets t_{n-1}$
\zi $f_z \gets \NOT(t_0 \IOR t_1 \IOR ... \IOR t_{n-1})$
\end{codebox}

\proc{inc} and \proc{dec} behave like \proc{add} and \proc{sub}, but with $b_0$ set to 1 and all other bits in $b$ set to 0.
Further, they don't modify $f_c$.
Once simplified, their implementations are as follows:

\begin{codebox}
\Procname{$\proc{inc}(a)$}
\zi $c_0 \gets a_0$
\zi $a_0 \gets \NOT a_0$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets a_i \AND c_{i-1}$
\zi   $a_i \gets a_i \XOR c_{i-1}$ \End
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\begin{codebox}
\Procname{$\proc{dec}(a)$}
\zi $c_0 \gets \NOT a_0$
\zi $a_0 \gets \NOT a_0$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets \NOT a_i \AND c_{i-1}$
\zi   $a_i \gets a_i \XOR c_{i-1}$ \End
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\proc{neg} performs two's-complement negation by logically negating each bit and adding 1.
Once simplified, its implementation is as follows:

\begin{codebox}
\Procname{$\proc{neg}(a)$}
\zi $c_0 \gets \NOT a_0$
\zi \For $i \gets 1$ \To $n-1$ \Do
\zi   $c_i \gets \NOT a_i \AND c_{i-1}$
\zi   $a_i \gets \NOT a_i \XOR c_{i-1}$ \End
\zi $f_c \gets a_0 + a_1 + ... + a_{n-1}$
\zi $f_o \gets c_{n-1} \XOR c_{n-2}$
\zi $f_p \gets \NOT(a_0 \XOR a_1 \XOR ... \XOR a_7)$
\zi $f_s \gets a_{n-1}$
\zi $f_z \gets \NOT(a_0 \IOR a_1 \IOR ... \IOR a_{n-1})$
\end{codebox}

\subsubsection{Multiplication instructions}

\proc{imul} is the most complex of the supported instructions, in particular when represented by BDDs.
Its implementation uses the \emph{shift-and-add} method of multiplication, which is similar to the common method of long multiplication illustrated in Figure~\ref{fig:long_multiplication}.

\begin{figure}
\centering
\include{long_multiplication}
\caption{Long multiplication of two 4-digit numbers.}
\label{fig:long_multiplication}
\end{figure}

This methods works particularly well with binary or Boolean representations, since multiplication by a single digit then becomes nothing more than a logical \emph{and}.
Multiplication of two $n$-bit numbers thus becomes little more than $n$ additions.
Further, the terms do not all need to be calculated before adding them, but can be calculated as needed.

In the implementation below, $a$ and $b$ are assumed to have been extended to $2n$ bits, the new bits set to 0.
Further, each bit in $p$ is assumed to be set to 0 initially.

% explain j-for loop over 32 additions, p+x

\begin{codebox}
\Procname{$\proc{imul}(a,b)$}
\zi \For $j \gets 0 \To n-1$ \Do
\zi   $x \gets a_{0} \AND b_{j}$
\zi   $c_{j} \gets p_{j} \AND x$
\zi   $p_{j} \gets p_{j} \XOR x$
\zi   \For $i \gets j+1 \To 2n-1$ \Do
\zi     $x \gets a_{i-j} \AND b_{j}$
\zi     $c_{i} \gets p_{i} \AND x \IOR p_{i} \AND c_{i-1} \IOR x \AND c_{i-1}$
\zi     $p_{i} \gets p_{i} \XOR x \XOR c_{i-1}$ \End \End
\zi \For $i \gets 0 \To n-1$ \Do
\zi   $a_{i} \gets p_{i}$ \End
\zi $f_c \gets f_o \gets p_{n} \IOR p_{n+1} \IOR ... \IOR p_{2n-1}$
\end{codebox}

As seen, the last step of the implementation is to set $a$ to the $n$ least significant bits of $p$.
$f_c$ and $f_o$ are then both set based on the remaining $n$ most significant bits of $p$.

However, since we really only need to know of any of the $n$ most significant bits of $p$ is 1, we can also implement the multiplication instruction as follows:

\begin{codebox}
\Procname{$\proc{imul}(a,b)$}
\zi \For $j \gets 0 \To n-1$ \Do
\zi   $x \gets a_{0} \AND b_{j}$
\zi   $c_{j} \gets p_{j} \AND x$
\zi   $p_{j} \gets p_{j} \XOR x$
\zi   \For $i \gets j+1 \To n-1$ \Do
\zi     $x \gets a_{i-j} \AND b_{j}$
\zi     $c_{i} \gets p_{i} \AND x \IOR p_{i} \AND c_{i-1} \IOR x \AND c_{i-1}$
\zi     $p_{i} \gets p_{i} \XOR x \XOR c_{i-1}$ \End
\zi   \For $i \gets n \To 2n-1$ \Do
\zi     $o \gets o \IOR a_{i-j} \AND b_{j}$ \End
\zi   $o \gets o \IOR c_{n-1}$ \End
\zi \For $i \gets 0 \To n-1$ \Do
\zi   $a_{i} \gets p_{i}$ \End
\zi $f_c \gets f_o \gets o$
\end{codebox}

Naturally, the equivalence of the two implementations was confirmed using the method described in this section.

\chapter{Results and conclusions}
\label{ch:results_conclusions}

\section{A superoptimized program}

The programs generated by the superoptimizer are often strange and seemingly obfuscated.

To illustrate, we shall examine a program that computes the \proc{sign} function, which takes a single number and determines if it is positive, negative, or equal to zero. % footnote: massalin
In pseudocode, \proc{sign} can be expressed as follows:

\begin{codebox}
\Procname{$\proc{sign}(a)$}
\zi \If $a > 0$
\zi \Do
      \Return $1$
    \End
\zi \If $a < 0$
\zi \Do
      \Return $-1$
    \End
\zi \Return $0$
\end{codebox}

For this function, the superoptimizer finds the following program, which computes \verb|r1| $\gets$ \proc{sign}(\verb|r0|):

\begin{verbatim}
add r0,r0; sbb r1,r1; sub r1,r0; adc r1,r0;
\end{verbatim}

At first, it is not obvious how this program works.
To understand it, one can step through it instruction by instruction,
for the three cases where \verb|r0| is initially positive, negative, or equal to zero,
while taking note of the values of \verb|r0|, \verb|r1| and \verb|cf| following the execution of each instruction,
as shown in Table~\ref{tab:sign_correct}.

\begin{table}
\centering
\begin{tabular}{l|lll}
& \verb|r0| > 0
& \verb|r0| < 0
& \verb|r0| = 0 \\
\hline
\verb|add r0,r0|
& \verb|r0| $\gets X$, \verb|cf| $\gets 0$ % note: r0 + r0 will never cause carry-out, because MSB of r0 is 0
& \verb|r0| $\gets Y$, \verb|cf| $\gets 1$
& \verb|r0| $\gets 0$, \verb|cf| $\gets 0$ \\
\verb|sbb r1,r1|
& \verb|r1| $\gets 0$
& \verb|r1| $\gets -1$
& \verb|r1| $\gets 0$ \\
\verb|sub r1,r0|
& \verb|r1| $\gets -X$, \verb|cf| $\gets 1$
& \verb|r1| $\gets -1-Y$, \verb|cf| $\gets 0$ % note: -1-Y will always be a negative number, because with two's complement, the range of negative numbers is 1 greater than the range of positive numbers
& \verb|r1| $\gets 0$, \verb|cf| $\gets 0$ \\
\verb|adc r1,r0|
& \verb|r1| $\gets 1$
& \verb|r1| $\gets -1$
& \verb|r1| $\gets 0$ \\
\end{tabular}
\caption{}
\label{tab:sign_correct}
\end{table}

For this function, the superoptimizer also finds a similar program, which passes the probabilistic test, but not the BDD test:

\begin{verbatim}
add r0,r0; sbb r1,r1; neg r0; adc r1,r1;
\end{verbatim}

This program does, in fact, behave like the first one for every input except one: the most negative 32-bit integer, which was not included in the test vector used by the superoptimizer's probabilistic test.
Using two's complement, this number is stored with its most significant bit set to 1, and all other bits set to 0.
For this number, the program behaves as shown in Table~\ref{tab:sign_incorrect}.

\begin{table}
\centering
\begin{tabular}{l|l}
\verb|add r0,r0|
& \verb|r0| $\gets 0$, \verb|cf| $\gets 1$ \\
\verb|sbb r1,r1|
& \verb|r1| $\gets -1$ \\
\verb|neg r0|
& \verb|r0| $\gets 0$, \verb|cf| $\gets 0$ \\
\verb|adc r1,r1|
& \verb|r1| $\gets -2$ \\
\end{tabular}
\caption{}
\label{tab:sign_incorrect}
\end{table}

While it could certainly be argued that the most negative 32-bit integer should have been included in the test vector, this program does help to illustrate that probabilistic testing is insufficient to fully determine the equivalence of two programs.

Since these two particular programs only have a single 32-bit input register, it is possible to test their behavior for \emph{every} possible input, as discussed in Section~\ref{ss:probabilistic_test}.
This was done, confirming the statements made about them above, in roughly 10 seconds on a 2.66 GHz processor.

\section{Performance of the BDD test on a few programs}
\label{s:performance_programs}

For the BDD test described in Section~\ref{ss:bdd_test} to be useful, it must be performant.
To determine the amount of time required to test two programs for equivalence, a few performance tests were performed.
While the results of such performance tests are highly dependent not only on the characteristics of BDDs, but also on specifics such as implementation and hardware, they do provide a general idea of how feasible it is to use BDDs for the purposes outlined in this thesis.

\subsection{Methodology}

The performance of the BDD test implementation was evaluated using a special version of the superoptimizer, with added timing functionality.
Two performance tests were performed,
the first measuring the time required to translate the 9 programs labelled $P_0$ to $P_8$ in Table~\ref{tab:test_programs} to SROBDD form,
and the second measuring the time required to test them for equivalence with each other.

All test programs have \verb|r0| and \verb|r1| as their input and output registers, respectively.
$P_0$ is equivalent to $P_3$ and $P_4$, as is $P_1$ to $P_2$, and $P_6$ to $P_8$.
$P_5$ and $P_7$ are not equivalent to any test program beside themselves.

$P_0$ provides a baseline measurement of the BDD test's overhead, as it consists of a single \verb|mov| instruction, which is implemented without any actual BDD operations.

\begin{table}
\centering
\begin{tabular}{l|l}
$P_0$ & \verb|mov r1,r0;| \\
$P_1$ & \verb|xor r1,r1; and r1,r0;| \\
$P_2$ & \verb|sub r1,r1; and r1,r0;| \\
$P_3$ & \verb|xor r1,r1; add r1,r0;| \\
$P_4$ & \verb|sub r1,r1; add r1,r0;| \\
$P_5$ & \verb|add r0,r0; sbb r1,r1; inc r1;| \\
$P_6$ & \verb|add r0,r0; sbb r1,r1; sub r1,r0; adc r1,r0;| \\
$P_7$ & \verb|add r0,r0; sbb r1,r1; neg r0; adc r1,r1;| \\
$P_8$ & \verb|xor r1,r1; xor r2,r2; xor r3,r3; inc r2; dec r3;|\\
      & \verb| cmp r1,r0; cmovl r1,r2; cmovg r1,r3;| \\
\end{tabular}
\caption{The programs used to test the performance of the BDD test.}
\label{tab:test_programs}
\end{table}

Since the time required to perform a single translation or equivalence test was too small to be measured with acceptable precision,
the tests instead measured the time required to perform 1000 translations or equivalence tests.

The performance tests were performed with the superoptimizer running on a 2.66 GHz Intel Core i7 processor.

% bdd_init(10000, 1000) used
% relevant?

\subsection{Results}

The results of the performance test measuring the time required to translate the programs to SROBDD form are shown in Table~\ref{tab:performance1}.
As expected, $P_0$ requires the least time.
The entries for $P_1$ -- $P_4$ illustrate the complexity of addition and subtraction instructions relative to logic instructions.
Looking at $P_4$ -- $P_7$, which all consist entirely of addition and subtraction instructions, one might infer that longer programs require more time.
However, this conclusion is weakened by the considerable difference between the entries for $P_6$ and $P_7$, which are of the same length,
as well as the entry for $P_8$, which is contains twice as many, albeit simpler, instructions as $P_6$, and requires less time.

\begin{table}
\centering
\begin{tabular}{*{9}{c}}
$P_0$   & $P_1$   & $P_2$   & $P_3$   & $P_4$   & $P_5$   & $P_6$   & $P_7$   & $P_8$   \\
\hline
0.119 s & 0.175 s & 0.284 s & 0.281 s & 0.389 s & 0.447 s & 0.966 s & 0.679 s & 0.673 s \\
\end{tabular}
\caption{The time required to translate the programs to SROBDD form 1000 times.}
\label{tab:performance1}
\end{table}

The results of the performance test measuring the time required to test two programs for equivalence are shown in Table~\ref{tab:performance2}.
The most time-consuming test is the one testing $P_6$ for equivalence with itself, although this still requires less than 2 seconds to be performed 1000 times.
The time required to test some programs $P_a$ and $P_b$ for equivalence, is roughly equal to the sum of the time required to translate the programs to SROBDDs form as seen in Table~\ref{tab:performance1}, minus the baseline time of $P_0$.

\begin{table}
\centering
\begin{tabular}{l|*{9}{c}}
      & $P_0$   & $P_1$   & $P_2$   & $P_3$   & $P_4$   & $P_5$   & $P_6$   & $P_7$   & $P_8$   \\
\hline
$P_0$ & 0.116 s & 0.176 s & 0.284 s & 0.281 s & 0.389 s & 0.447 s & 0.965 s & 0.679 s & 0.670 s \\
$P_1$ &         & 0.235 s & 0.343 s & 0.340 s & 0.449 s & 0.507 s & 1.025 s & 0.757 s & 0.729 s \\
$P_2$ &         &         & 0.468 s & 0.449 s & 0.553 s & 0.613 s & 1.131 s & 0.845 s & 0.837 s \\
$P_3$ &         &         &         & 0.422 s & 0.529 s & 0.592 s & 1.107 s & 0.821 s & 0.834 s \\
$P_4$ &         &         &         &         & 0.636 s & 0.698 s & 1.212 s & 0.927 s & 0.941 s \\
$P_5$ &         &         &         &         &         & 0.752 s & 1.270 s & 0.984 s & 1.001 s \\
$P_6$ &         &         &         &         &         &         & 1.781 s & 1.535 s & 1.530 s \\
$P_7$ &         &         &         &         &         &         &         & 1.171 s & 1.224 s \\
$P_8$ &         &         &         &         &         &         &         &         & 1.155 s \\
\end{tabular}
\caption{The time required to test the programs for equivalence with each other 1000 times.}
\label{tab:performance2}
\end{table}

% conclusions?

\section{Performance of the BDD test on programs performing multiplication}
\label{s:performance_mul}

While the performance tests described in Section~\ref{s:performance_programs} yielded very promising results, it should be noted that none of the programs tested contained any multiplication instructions.
The reason for this is unfortunately that translating even a single 32-bit multiplication instruction to SROBDD form turned out to take an unreasonable amount of time.

To gain some insight into what the performance would be like with fewer bits, a third performance test was performed, this time using a version of the superoptimizer supporting an architecture with a variable number of bits per registers, which did require some manual configuration of the BDD test.
Further, the test measured the time required to translate a multiplication instruction to SROBDD form only once, rather than 1000 times as in the previous test.

\subsection{Results}

Even with the restrictions mentioned above, the results of the multiplication performance test were disappointing, as shown in Figure~\ref{fig:performance_mul}.

\begin{figure}
\centering
\include{performance_mul}
\caption{The time required to translate a single $n$-bit multiplication instruction to SROBDD form as a function of $n$.}
\label{fig:performance_mul}
\end{figure}

% conclusions?

\chapter{Recommendations and future work}
\label{ch:recommendations_future_work}

It is hard to give an unconditional recommendation of the method of using BDDs to determine program equivalence described in this thesis, due to its poor performance when handling multiplication and similar instructions.

The performance could perhaps be improved by replacing the shift-and-add implementation of the multiplication instruction with a more efficient algorithm.
While the resulting SROBDD would be identical, a more efficient algorithm might require fewer operations to construct it.
% doubtful if improvement for such small values of n (32)

Another possibility to consider is using 8-bit or perhaps even 4-bit arithmetic for the BDD test.
The only architectural feature currently supported by the superoptimizer for which this would not work, is the parity flag, which is defined specifically by the 8 least significant bits of a result.
Since the parity flag is of limited utility, removing support for it would perhaps not be an issue.
A more serious problem, however, is that adding support for immediate values would become difficult.
It should also be noted, that with 8-bit or 4-bit arithmetic, testing programs with all possible input values as described in Section~\ref{ss:probabilistic_test} is feasible even for programs with many input registers.

Even without support for multiplication and similar instructions, the method of using BDDs to determine program equivalence described in this thesis could be useful.
There are many simple but important instructions, such as shift and rotate, for which support could be added.
One could also add support for programs with multiple output registers with relative ease.

As a final though, the BDD test might be best utilized stripped of the program generator and the probabilistic test, intended to be used as an external verification tool for a more capable superoptimizer or similar piece of software.

\bibliographystyle{ieeetr}
\bibliography{thesis}

\end{document}
