\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage{gnuplot-lua-tikz}

\usepackage{hyperref}
\hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

\usepackage{tikz}
\usetikzlibrary{positioning}

\title{Using binary decision diagrams to determine program equivalence in a superoptimizer}
\subtitle{}
\foreigntitle{Att använda binära beslutsdiagram för att avgöra ekvivalens mellan program i en superoptimerare}
\author{Jesper Särnesjö}
\date{}
\blurb{Master's Thesis at CSC \\ Supervisor: Torbjörn Granlund \\ Examiner: Johan Håstad}
\trita{TRITA xxx yyyy-nn}

\begin{document}

\frontmatter

\pagestyle{empty}

\removepagenumbers

\maketitle

\selectlanguage{english}

\begin{abstract}
\end{abstract}

\clearpage

\begin{foreignabstract}{swedish}
\end{foreignabstract}

\clearpage

\tableofcontents*

\mainmatter

\pagestyle{newchap}

\chapter{Introduction}
\label{ch:introduction}

This thesis explores using \emph{binary decision diagrams} (BDDs) to determine whether two programs are \emph{equivalent}, that is, whether they always yield identical output given identical and valid input, within the context of a \emph{superoptimizer}.

Chapter~\ref{ch:background} describes both superoptimizers and BDDs in detail, and summarizes their history.

Chapter~\ref{ch:design} describes the superoptimizer implemented for this thesis project.

\chapter{Background}
\label{ch:background}

\section{Superoptimizers}

\subsection{Massalin's superoptimizer}

The term \emph{superoptimizer} was coined by Massalin in a 1987 paper \cite{massalin87}, to describe a tool for finding the shortest loop-free program that computes a given function.

Massalin's implementation takes as its input a program written in assembly language for the Motorola 68020 processor.
It then consults a table containing a subset of the processor's instruction set, and begins generating all combinations of these instructions, beginning with those of length 1, then 2, and so on.
Each generated program is tested for equivalence with the input program.
When a program passes the test, it is printed, and the search terminates.
Because programs are tested in order of increasing length, this program will necessarily be \emph{optimal} in terms of length, meaning that no shorter equivalent program exists.

Naturally, exhaustively searching the space of all possible programs is a very inefficient process, considering that the number of programs of length $n$ is $b^n$, where $b$ is the \emph{branching factor} of the search tree.
To find a value of $b$, consider an architecture with $r$ registers, and an instruction set consisting of $i_0$ instructions that take no arguments, $i_1$ instructions that take one argument, and $i_2$ instructions that take two arguments.
For this architecture, the branching factor would be, in the worst case, $i_0+i_1r+i_2r^2$.
Even for modest values of $i_0$, $i_1$, $i_2$ and $r$, the branching factor ends up in the hundreds or thousands.
Hence, a naive superoptimizer is only capable of generating very short programs in reasonable time.

To address this, Massalin's superoptimizer has a method of \emph{pruning} the search tree.
The method exploits the fact that when optimizing for program length, an optimal sequence must not contain any non-optimal subsequences.
Hence, a program which contains a known non-optimal subsequence can be rejected without being tested.
The implementation consists of $m$-dimensional lookup tables, where $m$ is the length of the subsequence.

Massalin also describes two methods of determining whether a candidate program is equivalent to the input program.

The first method, referred to as the \emph{Boolean test}, is to express both programs in Boolean logic, reduce them to an unspecified canonical form, and compare them minterm for minterm.
Using this method, Massalin's superoptimizer is capable of testing 40 programs per second.

The second method, referred to as the \emph{probabilistic test}, is to simply execute both programs with identical input, and compare their output.
This method is significantly faster, allowing the superoptimizer to test 50000 programs per second.
However, it introduces the risk of false positives, in the form of programs that are equivalent only on the input used, not in the general case.
Hence, manual verification of the output is required.
Further, whereas the Boolean test can be used on any kind of program, the probabilistic test can only be used on programs written in the assembly language of the host architecture.

It is worth noting, that even with such a significant increase in speed, only slightly longer programs may be generated in reasonable time with a realistic branching factor, as illustrated by Figure~\ref{fig:so_program_length}.

\begin{figure}
\centering
\include{so_program_length}
\caption{The maximum length of a program generated in 24 hours, as a function of the branching factor. % clarify
The solid and dashed lines represent Massalin's superoptimizer using the Boolean and probabilistic tests, respectively.}
\label{fig:so_program_length}
\end{figure}

\subsection{GSO}

GSO, the GNU superoptimizer, is described in a 1992 paper by Granlund and Kenner \cite{granlund92}.

Like Massalin's superoptimizer, GSO optimizes for program length, and uses a probabilistic equivalence test.
However, instead of executing instructions directly on the host architecture, it \emph{simulates} them using functions that operate on a virtual machine of sorts, consisting of a set of registers and a single carry bit.
GSO is therefore capable of supporting a wide range of architectures, but also requires that the input program be specified as a compiled-in \emph{goal function}.

Granlund and Kenner also describe numerous methods of pruning used in GSO.
When selecting arguments for generated instructions, GSO only considers live registers, that is, registers that contain input values or have previously been written to.
Similarly, instructions that read the carry flag are only generated after it has been set.
For commutative instructions, which exist on some architectures, only one argument ordering is tried.

When generating the last instruction in a program, GSO is even more restrictive.
Specifically, it requires that the last instruction reads a register or the carry flag written to by the preceding instruction, which would otherwise be superfluous.

\subsection{Denali}

Denali is a superoptimizer created by Joshi et al, then at Compaq Systems Research Center, first described in a 2002 paper \cite{joshi02}.

Denali's design differs greatly from that of Massalin's superoptimizer or GSO.
Rather than generating programs and then testing them, Denali starts by generating a set of programs equivalent to the input program, and then selects the optimal among them.

To accomplish the first step, Denali requires a set of \emph{axioms}, which describe how instructions may be substituted for another without changing the meaning of the program.
Such axioms may, for example, state that multiplication by $2^n$ is the same as right-shift by $n$.
Axioms are also used to mark instructions as commutative or associative, and to specify their identity, for instance 0 for addition and 1 for multiplication.

To select the optimal program, Denali rewrites the problem as a satisfaction problem, which is handed off to an external SAT solver.

The authors claim that Denali is capable of optimizing for \emph{execution time}, that is, the number of cycles required for the program to terminate.
This is remarkable, as determining the execution time for a program is more complicated than it may seem.
The number of cycles required to execute an instruction, depends not only on the instruction itself, but also possibly on the instructions surrounding it, due to pipelining.
In a follow-up paper \cite{joshi06}, the authors describe a simplified design, named Denali-2, which instead optimizes for program length.

% input in somewhat C-like DSL
% multiple assignments
% Alpha EV-6

\subsection{Bansal's superoptimizer}

The superoptimizer created by Bansal and described in his 2008 PhD thesis \cite{bansal_thesis} differs from the ones described above, in that it requires no human supervision, neither for verifying its output, nor for selecting its input.
Instead, it reads binaries compiled for the x86 architecture, harvesting instruction sequences to optimize.
Its output is not an optimized program, but rather the optimizations discovered.
In this way, Bansal's superoptimizer is capable of autonomously generating a very large library of optimizations, which can then be used by an ordinary peephole optimizer.

Since output is required to be correct, optimizations are verified by a Boolean test performed by a SAT solver.
To improve speed, candidate optimizations are first screened by a probabilistic test similar to that found in Massalin's superoptimizer or GSO.

Other notable features of Bansal's superoptimizer is that it is capable of optimizing both for execution time and for program size.
It also utilizes a meet-in-the-middle method of pruning, in which instructions are prohibited from overwriting values required at program termination.

\subsection{TOAST}

TOAST, short for \emph{total optimisation using answer set technology}, is a superoptimizer created by Crick and described in his 2009 PhD thesis \cite{crick_thesis}.
It is implemented using Answer Set Programming, a declarative logic programming language.
Like Denali and Bansal's superoptimizer, it produces provably correct output.

%\subsection{...}
%
%\cite{aha}
%\cite{pic}

\section{Binary decision diagrams}

\emph{Binary decision diagrams} (BDDs) are data structures used to efficiently represent Boolean functions.
They consist of nodes, representing the function's variables. Each node has one or more incoming paths, and exactly two outgoing paths, commonly referred to as the \emph{high} and \emph{low} path.
When using a BDD to determine the value of a Boolean function, one follows a node's high path if its variable is 1, and its low path if its variable is 0, starting at the root node, until one arrives at a terminal node.

BDDs were introduced by Lee \cite{lee59}, under the name \emph{binary-decision programs}.
They were given their current name by Akers \cite{akers78}, who also explored the ideas of \emph{reducing} BDDs by removing redundant nodes, and of representing multiple functions in a single BDD.

A more restricted type of BDD was introduced by Bryant \cite{bryant86}.
In addition to the above, Bryant required the BDD to be \emph{ordered}, having its variables appear in the same order on all paths from the root to a terminal node,
and to be \emph{reduced}, containing no node with the same high and low path, and no two distinct nodes with isomorphic subgraphs.
Such a BDD is called a \emph{reduced ordered BDD} (ROBDD), and has several beneficial characteristics.
First, the algorithms used to perform Boolean operations on one or a pair of ROBDDs, are efficient, requiring at worst a number of time steps proportional to the product of the number of nodes in the two ROBDDs.
Further, ROBDDs are \emph{canonical} representations of their Boolean functions, meaning that two Boolean functions are equivalent iff their ROBDDs are isomorphic.

Bryant also further explored the idea of representing multiple functions in a single BDD, which would later be referred to as a \emph{shared BDD} by Minato et al \cite{minato90}.

A BDD with all these characteristics, a \emph{shared reduced ordered BDD} (SROBDD), has the added benefits of being memory efficient, and of allowing an isomorphism test to be implemented as a single pointer comparison.

Bryant showed that the variable ordering used can have a dramatic effect on the number of nodes in a BDD, which in the best case is linear in the number of variables, and in the worst case exponential.
Bryant states that finding the variable ordering that minimizes the number of nodes is a NP-complete problem.
Bollig and Wegener \cite{bollig96} would later show that even improving on an existing ordering is a NP-complete problem.

% asserts that for many problems, good order can be selected by human with domain knowledge
% however, functions exists (e.g. multiplication) that require an exponential number of nodes regardless of ordering

\subsection{Example}

To illustrate how a moderately complex Boolean function can be represented by a BDD, consider a class of functions with $2n$ variables labeled $a_0,...,a_{n-1},b_0,...,b_{n-1}$, defined recursively as follows:

$$
  c_k = \left\{
  \begin{array}{ll}
    a_k b_k                             & k = 0 \\
    a_k b_k + a_k c_{k-1} + b_k c_{k-1} & 0 < k < n \\
  \end{array}\right.
$$

$c_{n-1}$ is the function for the carry-out bit of an $n$-bit adder with no carry-in, taking $a$ and $b$ as its two inputs.
Its truth table is listed in Table~\ref{tab:tt_c1}, and it is shown in OBDD and ROBDD form in Figure~\ref{fig:bdd_c1}, using the variable ordering $a_0,b_0,...,a_{n-1},b_{n-1}$.
If we instead use the variable ordering $a_0,...,a_{n-1},b_0,...,b_{n-1}$, we end up with a larger ROBDD, as illustrated in Figure~\ref{fig:bdd_c2_bad}.
For this particular function, and these two variable orderings, the number of nodes required is:

$$
  |c_{n-1}| = \left\{
  \begin{array}{ll}
    3n-1      & \textrm{using variable order $a_0,b_0,...,a_{n-1},b_{n-1}$} \\
    2^{n+1}-2 & \textrm{using variable order $a_0,...,a_{n-1},b_0,...,b_{n-1}$} \\
  \end{array}\right.
$$

For $n = 32$, the BDD would consequently contain only 97 nodes with the first variable ordering, but over 8 billion with the second.

\begin{table}
\centering
\begin{tabular}{cccc|cc}
$a_0$ & $b_0$ & $a_1$ & $b_1$ & $c_0$ & $c_1$ \\
\hline
0     & 0     & 0     & 0     & 0     & 0 \\
0     & 0     & 0     & 1     & 0     & 0 \\
0     & 0     & 1     & 0     & 0     & 0 \\
0     & 0     & 1     & 1     & 0     & 1 \\
0     & 1     & 0     & 0     & 0     & 0 \\
0     & 1     & 0     & 1     & 0     & 0 \\
0     & 1     & 1     & 0     & 0     & 0 \\
0     & 1     & 1     & 1     & 0     & 1 \\
1     & 0     & 0     & 0     & 0     & 0 \\
1     & 0     & 0     & 1     & 0     & 0 \\
1     & 0     & 1     & 0     & 0     & 0 \\
1     & 0     & 1     & 1     & 0     & 1 \\
1     & 1     & 0     & 0     & 1     & 0 \\
1     & 1     & 0     & 1     & 1     & 1 \\
1     & 1     & 1     & 0     & 1     & 1 \\
1     & 1     & 1     & 1     & 1     & 1 \\
\end{tabular}
\caption{Truth table for the Boolean function $c_1$.}
\label{tab:tt_c1}
\end{table}

\begin{figure}
\centering
\include{bdd_c1}
\caption{The Boolean function $c_1$ as an OBDD (left) and an ROBDD (right). The solid lines represent high paths and the dashed lines represent low paths.}
\label{fig:bdd_c1}
\end{figure}

\begin{figure}
\centering
\include{bdd_c2_bad}
\caption{The Boolean function $c_2$ in ROBDD form using the variable order $a_0,a_1,a_2,b_0,b_1,b_2$ (left) and $a_0,b_0,a_1,b_1,a_2,b_2$ (right).}
\label{fig:bdd_c2_bad}
\end{figure}

\chapter{Design}
\label{ch:design}

As a framework for exploring the use of BDDs to determine program equivalence, I developed a simple superoptimizer.
Its implementation consists of roughly 2000 lines of C and C++ code, as well as a test suite created to aid development.
Its design is inspired primarily by GSO.
Since my focus was on equivalence testing, the other aspects of the design were mostly kept simple.
It optimizes for program length.

The architecture of the superoptimizer's virtual machine is modelled after the x86, as described by the manuals issued by Intel Corporation \cite{intel_1,intel_2a,intel_2b}. % AMD?
The machine state consists of 8 32-bit registers and 5 flag bits.
The \emph{carry flag} (CF) and \emph{overflow flag} (OF) are ostensibly used to signify that an operation caused overflow when interpreted as unsigned and signed arithmetic, respectively, although many instructions treat them differently.
The \emph{parity flag} (PF) is set when an even number of the least significant 8 bits of the result are set.
The \emph{sign flag} (SF) is set equal to the most significant bit of the result.
The \emph{zero flag} (ZF) is set when the result contains no set bits.

The superoptimizer takes as its input programs written in a small subset of the x86 assembly language, using Intel's syntax with a few modifications.
Registers are named r0--r7, rather than eax, ebx, etc.
Statements are terminated by semicolons rather than newlines.
Neither immediate values nor effective addresses are supported.
Table~\ref{tab:insns} shows the supported instructions, along with their arity and behavior with regards to flags.

\begin{table}
\centering
\begin{tabular}{lc|ccccc}
name & arity & CF & OF & PF & SF & ZF \\
\hline
stc  & 0     & 1  &    &    &    &    \\
clc  & 0     & 0  &    &    &    &    \\
cmc  & 0     & TM &    &    &    &    \\
mov  & 2     &    &    &    &    &    \\
cmov & 2     & T  & T  & T  & T  & T  \\
and  & 2     & 0  & 0  & M  & M  & M  \\
or   & 2     & 0  & 0  & M  & M  & M  \\
xor  & 2     & 0  & 0  & M  & M  & M  \\
not  & 1     &    &    &    &    &    \\
add  & 2     & M  & M  & M  & M  & M  \\
adc  & 2     & TM & M  & M  & M  & M  \\
sub  & 2     & M  & M  & M  & M  & M  \\
sbb  & 2     & TM & M  & M  & M  & M  \\
cmp  & 2     & M  & M  & M  & M  & M  \\
inc  & 1     &    & M  & M  & M  & M  \\
dec  & 1     &    & M  & M  & M  & M  \\
neg  & 1     & M  & M  & M  & M  & M  \\
\end{tabular}
\caption{The instruction set of the superoptimizer.
For the flags, 0 and 1 means that it is set to that value, M (modify) means that it is set according the result, and T (test) means that it is read.}
\label{tab:insns}
\end{table}

The superoptimizer analyzes the registers used in the input program.
Any register read from before written to is considered an input register.
The last register written to is marked as the output register.
Only a single output register is supported.

For pruning, the generator keeps track of the set of live registers.
Initially, this is equal to the set of input registers, and registers are added when written to.
Only registers in the live set are considered when selecting arguments for instructions.
Live flags are tracked in a similar way.

% cmov a != b
% leaf round

Instructions are modelled in three ways.

First, the superoptimizer has a lookup table that specifies which of an instruction's arguments, and which flags, are read from or written to.
This is used by the generator to update the live sets, and to avoid generating instructions that depend on flags not yet defined.

Second, the instructions' behavior is implemented in functions used in a probabilistic test.

Finally, and most importantly, the instructions' behavior is modelled as operations on BDDs, for use in the Boolean test.
For this, the software package BuDDy \cite{buddy} is used.

\section{BDD model}

The virtual machine's 8 32-bit registers are represented by $8 \times 32$ Boolean variables. % ordered r0b0,r1b0,...,r7b0,r1b0,etc
The flags are functions rather than variables.

The parity, sign and zero flags are defined in the same way for all instructions:

\begin{verbatim}
pf = !(a[0] ^ a[1] ^ a[2] ^ a[3] ^ a[4] ^ a[5] ^ a[6] ^ a[7])
sf = a[31]
zf = !(a[0] | a[1] | ... | a[31])
\end{verbatim}

\subsection{Flag control}

The simplest instructions supported by the superoptimizer are the flag control instructions \emph{stc}, \emph{clc} and \emph{cmc}.

\subsection{Data transfer}

The superoptimizer supports conditional and unconditional data transfer.
The \emph{mov} instruction is used for the latter, and is implemented as follows:

\begin{verbatim}
for i = 0 to 31
    a[i] = b[i]
\end{verbatim}

The \emph{cmov} instructions are used for conditional data transfer.
There are 30 \emph{cmov} instructions in the x86 instruction set, but only 16 with unique behavior.
They are all implemented as follows:

\begin{verbatim}
for i = 0 to 31
    a[i] = !condition & a[i] | condition & b[i]
\end{verbatim}

% cmovc
% cmovo
% cmovp
% cmovs
% cmovz
% cmovnc
% cmovno
% cmovnp
% cmovns
% cmovnz
% cmova   above
% cmovbe  below or equal
% cmovg   greater
% cmovge  greater or equal
% cmovl   less
% cmovle  less or equal

% cmovae  above or equal=nc
% cmovb   below=c
% cmovna  not above=be
% cmovnae not above or equal=c
% cmovnb  not below=nc
% cmovnbe not below or equal=a
% cmovng  not greater=le
% cmovnge not greater or equal=l
% cmovnl  not less=ge
% cmovnle not less or equal=g
% cmove   equal=z
% cmovne  not equal=zf
% cmovpe  parity even=p
% cmovpo  parity odd=np

% above/below for unsigned
% greater/less for signed

\subsection{Logic}

The supported instructions for bitwise logic are \emph{and}, \emph{or}, \emph{xor} and \emph{not}.
They are all simple, since each bit in the result depends only on the corresponding bit in the inputs.

\emph{and} is implemented as follows:

\begin{verbatim}
for i = 0 to 31
    a[i] = a[i] & b[i]
\end{verbatim}

\emph{or} and \emph{xor} are implemented similarly.

\emph{not} is implemented as follows:

\begin{verbatim}
for i = 0 to 31
    a[i] = !a[i]
\end{verbatim}

\subsection{Addition and subtraction}

The \emph{add}, \emph{adc}, \emph{sub}, \emph{sbb}, \emph{cmp}, \emph{inc}, \emph{dec} and \emph{neg} instructions are all variants of addition.

\emph{add} is implemented as follows:

\begin{verbatim}
c[0] = a[0] & b[0]
a[0] = a[0] ^ b[0]
for i = 1 to 31
    c[i] = a[i] & b[i] | a[i] & c[i-1] | b[i] & c[i-1]
    a[i] = a[i] ^ b[i] ^ c[i-1]
cf = c[31]
of = c[30] ^ c[31]
\end{verbatim}

For \emph{adc} and \emph{sbb}, \emph{cf} is taken into account when \emph{c0} and \emph{a0} are set.
For \emph{sub} and \emph{sbb}, the two's complement of \emph{b} is used.
\emph{cmp} behaves like \emph{sub}, except it does not modify \emph{a}.
\emph{inc} and \emph{dec} behave like \emph{add} and \emph{sub}, with fixed values for \emph{b}.

\emph{neg} behaves like \emph{inc}, with the values for \emph{a} inverted.
Further, it sets the carry flag if \emph{a} is non-zero:

\begin{verbatim}
cf = a[0] | a[1] | ... | a[31]
\end{verbatim}

\bibliographystyle{ieeetr}
\bibliography{thesis}

\end{document}
